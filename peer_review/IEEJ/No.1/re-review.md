著者らの修正済原稿と回答書を拝読した結果、各査読者の照会事項での問題点が適切に修正され、本論文の新規性が読者により伝わるように改善されたと思います。以上から、改めて本論文は掲載に値すると判断し、「掲載決定(判定A)」とします。

また、著者らの回答書を拝読する中で、査読者Bの一部の照会事項あるいはコメントについて、著者らが理解できなかったという記述がありました。
これを踏まえ、下記のコメントにて、可能な限り補足しておきます。
なお、このコメントは今回の掲載に影響するものではなく、今後の研究での活用を目的としたものです。
下記コメントを踏まえた、本論文の原稿修正は不要であることを強調しておきます。


●コメント1
Optunaはハイパラ調整だけでなく実際には一般ソルバとして使われてる、だから比較対象とすべきだと言った。


- Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama: “Optuna: A Next-generation Hyperparameter Optimization Framework,” In KDD (2019) (https://optuna.org/)


●コメント2
査読者Bコメント3において、「入出力変数に対する誤差の重み付けについて十分理解できない」という回答があったため、これを具体的に補足します。

一般的な回帰の最小二乗法(Ordinary Least Squares:OLS)は、目的変数に関する誤差E(w; u[s], d[s])=Σ_[s=1,…,S](x[s]-d[s])^2を最小化することで、x[s]=F
(u[s]; w)のモデルパラメータwを求める方法論です。ここで、(u, d)は学習データ、Fは代理モデル、xは代理モデルの出力値です。
これに対して、主成分分析(PCA、別名最小距離二乗法)は、データ点とモデルの最短距離、すなわち
E(w; u[s], d[s])=Σ_[s=1,…,S]ω(x[s]-d[s])^2を最小化することで、モデルパラメータwを求める方法論です。ここで、ω=Σ_[i=1,...,M] wi + 1で、Mは説明変数の個数です。

目的変数Yと主成分の共分散が最大

言い換えると、OLSは予測誤差(出力変数方向の誤差)だけを考慮しており、PCRは学習データとモデルの最短距離(入出力変数の両方向の誤差に重み付け)を考慮していることから、これらは入出力変数空間における誤差の方向の定義だけが異なっているとみなせます。

また、予測を目的としてモデリングする場合、OLSのように予測誤差を単に最小化する方法が一般的かつ合理的ですが、入出力変数を拘束する等式制約条件を満たすためにモデリングすると同時に目的関数を最小化する最適化(本論文の問題設定を含む)の場合、スカラの閾値に対して、入力方向と出力方向の誤差の価値の違いはケースバイケースであるため、必ずしも予測誤差(出力誤差)だけを考慮するのが合理的とは言えません。
このため、PCRのように、学習データの分散を最小化する正規直交基底から重みに換算する等、入力方向と出力方向の価値に応じた合理的な方向によって、出力誤差にこだわらずに誤差関数を定義することは、本論文の目的により合致した最適化に寄与するアイディアとなり得ると考え、コメントさせていただきました。
加えて、最適化での利用を目的とした入出力変数の拘束条件を求める場合、入力変数群の中でも、実体としてモデリング誤差に考慮するには価値が低い変数(外乱等)については、重みを小さくするように人為的に調整するなど、実応用を意識した工夫も考えられます。

しかしながら、PCRのように合理的な正規直交基底から重みを得て、入出力変数の誤差に反映する方法は、本論文が想定する、複雑な非線形モデルにおいては基本的に適用不可で、線形モデルだからこそ適用可能といえます。
したがって、本論文でも入出力の重み付け自体は可能ですが、線形モデルにおけるジャストアイデアだけを伝えておくのに留めておきます。


以上

